2023-04-03 17:01:12,023 ----------------------------------------------------------------------------------------------------
2023-04-03 17:01:12,027 Model: "TextClassifier(
  (embeddings): TransformerDocumentEmbeddings(
    (model): DistilBertModel(
      (embeddings): Embeddings(
        (word_embeddings): Embedding(30523, 768)
        (position_embeddings): Embedding(512, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (transformer): Transformer(
        (layer): ModuleList(
          (0-5): 6 x TransformerBlock(
            (attention): MultiHeadSelfAttention(
              (dropout): Dropout(p=0.1, inplace=False)
              (q_lin): Linear(in_features=768, out_features=768, bias=True)
              (k_lin): Linear(in_features=768, out_features=768, bias=True)
              (v_lin): Linear(in_features=768, out_features=768, bias=True)
              (out_lin): Linear(in_features=768, out_features=768, bias=True)
            )
            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (ffn): FFN(
              (dropout): Dropout(p=0.1, inplace=False)
              (lin1): Linear(in_features=768, out_features=3072, bias=True)
              (lin2): Linear(in_features=3072, out_features=768, bias=True)
              (activation): GELUActivation()
            )
            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          )
        )
      )
    )
  )
  (decoder): Linear(in_features=768, out_features=6, bias=True)
  (dropout): Dropout(p=0.0, inplace=False)
  (locked_dropout): LockedDropout(p=0.0)
  (word_dropout): WordDropout(p=0.0)
  (loss_function): CrossEntropyLoss()
  (weights): None
  (weight_tensor) None
)"
2023-04-03 17:01:12,027 ----------------------------------------------------------------------------------------------------
2023-04-03 17:01:12,027 Corpus: "Corpus: 3832 train + 959 dev + 1198 test sentences"
2023-04-03 17:01:12,027 ----------------------------------------------------------------------------------------------------
2023-04-03 17:01:12,027 Parameters:
2023-04-03 17:01:12,027  - learning_rate: "0.000050"
2023-04-03 17:01:12,027  - mini_batch_size: "4"
2023-04-03 17:01:12,027  - patience: "3"
2023-04-03 17:01:12,027  - anneal_factor: "0.5"
2023-04-03 17:01:12,027  - max_epochs: "10"
2023-04-03 17:01:12,027  - shuffle: "True"
2023-04-03 17:01:12,027  - train_with_dev: "False"
2023-04-03 17:01:12,027  - batch_growth_annealing: "False"
2023-04-03 17:01:12,027 ----------------------------------------------------------------------------------------------------
2023-04-03 17:01:12,027 Model training base path: "test_distilbert"
2023-04-03 17:01:12,027 ----------------------------------------------------------------------------------------------------
2023-04-03 17:01:12,027 Device: cuda:0
2023-04-03 17:01:12,027 ----------------------------------------------------------------------------------------------------
2023-04-03 17:01:12,027 Embeddings storage mode: none
2023-04-03 17:01:12,027 ----------------------------------------------------------------------------------------------------
2023-04-03 17:01:21,107 epoch 1 - iter 95/958 - loss 1.85814495 - time (sec): 9.08 - samples/sec: 41.85 - lr: 0.000005
2023-04-03 17:01:26,351 epoch 1 - iter 190/958 - loss 1.51240314 - time (sec): 14.32 - samples/sec: 53.06 - lr: 0.000010
2023-04-03 17:01:31,153 epoch 1 - iter 285/958 - loss 1.31106785 - time (sec): 19.13 - samples/sec: 59.60 - lr: 0.000015
2023-04-03 17:01:35,928 epoch 1 - iter 380/958 - loss 1.16603125 - time (sec): 23.90 - samples/sec: 63.59 - lr: 0.000020
2023-04-03 17:01:40,683 epoch 1 - iter 475/958 - loss 1.09591234 - time (sec): 28.66 - samples/sec: 66.30 - lr: 0.000025
2023-04-03 17:01:47,628 epoch 1 - iter 570/958 - loss 1.03552469 - time (sec): 35.60 - samples/sec: 64.04 - lr: 0.000030
2023-04-03 17:01:52,857 epoch 1 - iter 665/958 - loss 0.99009664 - time (sec): 40.83 - samples/sec: 65.15 - lr: 0.000035
2023-04-03 17:01:57,853 epoch 1 - iter 760/958 - loss 0.94576474 - time (sec): 45.83 - samples/sec: 66.34 - lr: 0.000040
2023-04-03 17:02:02,772 epoch 1 - iter 855/958 - loss 0.93089969 - time (sec): 50.75 - samples/sec: 67.40 - lr: 0.000045
2023-04-03 17:02:07,877 epoch 1 - iter 950/958 - loss 0.88643487 - time (sec): 55.85 - samples/sec: 68.04 - lr: 0.000050
2023-04-03 17:02:08,306 ----------------------------------------------------------------------------------------------------
2023-04-03 17:02:08,309 EPOCH 1 done: loss 0.8857 - lr 0.000050
2023-04-03 17:02:13,069 Evaluating as a multi-label problem: False
2023-04-03 17:02:13,077 DEV : loss 0.6004646420478821 - f1-score (micro avg)  0.8123
2023-04-03 17:02:13,085 ----------------------------------------------------------------------------------------------------
2023-04-03 17:02:18,135 epoch 2 - iter 95/958 - loss 0.45843776 - time (sec): 5.05 - samples/sec: 75.25 - lr: 0.000049
2023-04-03 17:02:23,234 epoch 2 - iter 190/958 - loss 0.54511094 - time (sec): 10.15 - samples/sec: 74.89 - lr: 0.000049
2023-04-03 17:02:28,272 epoch 2 - iter 285/958 - loss 0.51596503 - time (sec): 15.19 - samples/sec: 75.06 - lr: 0.000048
2023-04-03 17:02:33,979 epoch 2 - iter 380/958 - loss 0.51920980 - time (sec): 20.89 - samples/sec: 72.75 - lr: 0.000048
2023-04-03 17:02:38,951 epoch 2 - iter 475/958 - loss 0.49302458 - time (sec): 25.87 - samples/sec: 73.46 - lr: 0.000047
2023-04-03 17:02:44,295 epoch 2 - iter 570/958 - loss 0.50107046 - time (sec): 31.21 - samples/sec: 73.05 - lr: 0.000047
2023-04-03 17:02:49,598 epoch 2 - iter 665/958 - loss 0.51876758 - time (sec): 36.51 - samples/sec: 72.85 - lr: 0.000046
2023-04-03 17:02:57,893 epoch 2 - iter 760/958 - loss 0.52079169 - time (sec): 44.81 - samples/sec: 67.85 - lr: 0.000046
2023-04-03 17:03:02,893 epoch 2 - iter 855/958 - loss 0.50926973 - time (sec): 49.81 - samples/sec: 68.66 - lr: 0.000045
2023-04-03 17:03:08,727 epoch 2 - iter 950/958 - loss 0.50103228 - time (sec): 55.64 - samples/sec: 68.29 - lr: 0.000044
2023-04-03 17:03:09,135 ----------------------------------------------------------------------------------------------------
2023-04-03 17:03:09,135 EPOCH 2 done: loss 0.5005 - lr 0.000044
2023-04-03 17:03:13,792 Evaluating as a multi-label problem: False
2023-04-03 17:03:13,798 DEV : loss 0.5586318373680115 - f1-score (micro avg)  0.8311
2023-04-03 17:03:13,802 ----------------------------------------------------------------------------------------------------
2023-04-03 17:03:18,766 epoch 3 - iter 95/958 - loss 0.26816318 - time (sec): 4.96 - samples/sec: 76.61 - lr: 0.000044
2023-04-03 17:03:24,359 epoch 3 - iter 190/958 - loss 0.25836823 - time (sec): 10.55 - samples/sec: 72.02 - lr: 0.000043
2023-04-03 17:03:32,682 epoch 3 - iter 285/958 - loss 0.28218816 - time (sec): 18.88 - samples/sec: 60.39 - lr: 0.000043
2023-04-03 17:03:40,944 epoch 3 - iter 380/958 - loss 0.29141863 - time (sec): 27.14 - samples/sec: 56.01 - lr: 0.000042
2023-04-03 17:03:49,844 epoch 3 - iter 475/958 - loss 0.29106275 - time (sec): 36.04 - samples/sec: 52.72 - lr: 0.000042
2023-04-03 17:03:58,423 epoch 3 - iter 570/958 - loss 0.28675467 - time (sec): 44.62 - samples/sec: 51.10 - lr: 0.000041
2023-04-03 17:04:06,437 epoch 3 - iter 665/958 - loss 0.29186919 - time (sec): 52.63 - samples/sec: 50.54 - lr: 0.000041
2023-04-03 17:04:14,513 epoch 3 - iter 760/958 - loss 0.30512612 - time (sec): 60.71 - samples/sec: 50.08 - lr: 0.000040
2023-04-03 17:04:22,857 epoch 3 - iter 855/958 - loss 0.30614113 - time (sec): 69.05 - samples/sec: 49.53 - lr: 0.000039
2023-04-03 17:04:30,693 epoch 3 - iter 950/958 - loss 0.30161473 - time (sec): 76.89 - samples/sec: 49.42 - lr: 0.000039
2023-04-03 17:04:31,006 ----------------------------------------------------------------------------------------------------
2023-04-03 17:04:31,006 EPOCH 3 done: loss 0.3000 - lr 0.000039
2023-04-03 17:04:33,037 Evaluating as a multi-label problem: False
2023-04-03 17:04:33,044 DEV : loss 0.7546265721321106 - f1-score (micro avg)  0.8488
2023-04-03 17:04:33,048 ----------------------------------------------------------------------------------------------------
2023-04-03 17:04:36,582 epoch 4 - iter 95/958 - loss 0.09391454 - time (sec): 3.53 - samples/sec: 107.55 - lr: 0.000038
2023-04-03 17:04:40,129 epoch 4 - iter 190/958 - loss 0.09442980 - time (sec): 7.08 - samples/sec: 107.34 - lr: 0.000038
2023-04-03 17:04:43,668 epoch 4 - iter 285/958 - loss 0.10349272 - time (sec): 10.62 - samples/sec: 107.34 - lr: 0.000037
2023-04-03 17:04:47,282 epoch 4 - iter 380/958 - loss 0.09403618 - time (sec): 14.23 - samples/sec: 106.79 - lr: 0.000037
2023-04-03 17:04:51,015 epoch 4 - iter 475/958 - loss 0.09824828 - time (sec): 17.97 - samples/sec: 105.75 - lr: 0.000036
2023-04-03 17:04:54,641 epoch 4 - iter 570/958 - loss 0.10209063 - time (sec): 21.59 - samples/sec: 105.59 - lr: 0.000036
2023-04-03 17:04:58,359 epoch 4 - iter 665/958 - loss 0.10679017 - time (sec): 25.31 - samples/sec: 105.10 - lr: 0.000035
2023-04-03 17:05:02,037 epoch 4 - iter 760/958 - loss 0.11087171 - time (sec): 28.99 - samples/sec: 104.87 - lr: 0.000034
2023-04-03 17:05:05,737 epoch 4 - iter 855/958 - loss 0.11283758 - time (sec): 32.69 - samples/sec: 104.62 - lr: 0.000034
2023-04-03 17:05:09,299 epoch 4 - iter 950/958 - loss 0.11104920 - time (sec): 36.25 - samples/sec: 104.83 - lr: 0.000033
2023-04-03 17:05:09,602 ----------------------------------------------------------------------------------------------------
2023-04-03 17:05:09,602 EPOCH 4 done: loss 0.1104 - lr 0.000033
2023-04-03 17:05:11,853 Evaluating as a multi-label problem: False
2023-04-03 17:05:11,860 DEV : loss 1.0739132165908813 - f1-score (micro avg)  0.8425
2023-04-03 17:05:11,865 ----------------------------------------------------------------------------------------------------
2023-04-03 17:05:15,506 epoch 5 - iter 95/958 - loss 0.00819609 - time (sec): 3.64 - samples/sec: 104.50 - lr: 0.000033
2023-04-03 17:05:19,123 epoch 5 - iter 190/958 - loss 0.02203445 - time (sec): 7.25 - samples/sec: 104.78 - lr: 0.000032
2023-04-03 17:05:22,678 epoch 5 - iter 285/958 - loss 0.01994463 - time (sec): 10.81 - samples/sec: 105.47 - lr: 0.000032
2023-04-03 17:05:26,239 epoch 5 - iter 380/958 - loss 0.02158845 - time (sec): 14.37 - samples/sec: 105.78 - lr: 0.000031
2023-04-03 17:05:29,808 epoch 5 - iter 475/958 - loss 0.03211038 - time (sec): 17.94 - samples/sec: 105.92 - lr: 0.000031
2023-04-03 17:05:33,401 epoch 5 - iter 570/958 - loss 0.04069776 - time (sec): 21.53 - samples/sec: 105.89 - lr: 0.000030
2023-04-03 17:05:36,985 epoch 5 - iter 665/958 - loss 0.04083553 - time (sec): 25.12 - samples/sec: 105.91 - lr: 0.000029
2023-04-03 17:05:40,596 epoch 5 - iter 760/958 - loss 0.04125630 - time (sec): 28.73 - samples/sec: 105.83 - lr: 0.000029
2023-04-03 17:05:44,271 epoch 5 - iter 855/958 - loss 0.04599500 - time (sec): 32.40 - samples/sec: 105.55 - lr: 0.000028
2023-04-03 17:05:47,857 epoch 5 - iter 950/958 - loss 0.04823950 - time (sec): 35.99 - samples/sec: 105.59 - lr: 0.000028
2023-04-03 17:05:48,153 ----------------------------------------------------------------------------------------------------
2023-04-03 17:05:48,153 EPOCH 5 done: loss 0.0478 - lr 0.000028
2023-04-03 17:05:50,203 Evaluating as a multi-label problem: False
2023-04-03 17:05:50,210 DEV : loss 1.1894316673278809 - f1-score (micro avg)  0.8446
2023-04-03 17:05:50,215 ----------------------------------------------------------------------------------------------------
2023-04-03 17:05:53,760 epoch 6 - iter 95/958 - loss 0.01172948 - time (sec): 3.54 - samples/sec: 107.20 - lr: 0.000027
2023-04-03 17:05:57,458 epoch 6 - iter 190/958 - loss 0.01669235 - time (sec): 7.24 - samples/sec: 104.94 - lr: 0.000027
2023-04-03 17:06:01,104 epoch 6 - iter 285/958 - loss 0.01276729 - time (sec): 10.89 - samples/sec: 104.70 - lr: 0.000026
2023-04-03 17:06:04,696 epoch 6 - iter 380/958 - loss 0.01873595 - time (sec): 14.48 - samples/sec: 104.97 - lr: 0.000026
2023-04-03 17:06:08,263 epoch 6 - iter 475/958 - loss 0.02127126 - time (sec): 18.05 - samples/sec: 105.28 - lr: 0.000025
2023-04-03 17:06:11,929 epoch 6 - iter 570/958 - loss 0.01775567 - time (sec): 21.71 - samples/sec: 105.00 - lr: 0.000024
2023-04-03 17:06:15,586 epoch 6 - iter 665/958 - loss 0.02274572 - time (sec): 25.37 - samples/sec: 104.85 - lr: 0.000024
2023-04-03 17:06:19,121 epoch 6 - iter 760/958 - loss 0.02011987 - time (sec): 28.91 - samples/sec: 105.17 - lr: 0.000023
2023-04-03 17:06:22,761 epoch 6 - iter 855/958 - loss 0.02255030 - time (sec): 32.55 - samples/sec: 105.08 - lr: 0.000023
2023-04-03 17:06:26,406 epoch 6 - iter 950/958 - loss 0.02047187 - time (sec): 36.19 - samples/sec: 105.00 - lr: 0.000022
2023-04-03 17:06:26,711 ----------------------------------------------------------------------------------------------------
2023-04-03 17:06:26,711 EPOCH 6 done: loss 0.0203 - lr 0.000022
2023-04-03 17:06:28,781 Evaluating as a multi-label problem: False
2023-04-03 17:06:28,788 DEV : loss 1.3934829235076904 - f1-score (micro avg)  0.8551
2023-04-03 17:06:28,794 ----------------------------------------------------------------------------------------------------
2023-04-03 17:06:32,312 epoch 7 - iter 95/958 - loss 0.00809599 - time (sec): 3.52 - samples/sec: 108.01 - lr: 0.000022
2023-04-03 17:06:35,849 epoch 7 - iter 190/958 - loss 0.01500946 - time (sec): 7.05 - samples/sec: 107.73 - lr: 0.000021
2023-04-03 17:06:39,426 epoch 7 - iter 285/958 - loss 0.01735367 - time (sec): 10.63 - samples/sec: 107.22 - lr: 0.000021
2023-04-03 17:06:43,051 epoch 7 - iter 380/958 - loss 0.01361546 - time (sec): 14.26 - samples/sec: 106.62 - lr: 0.000020
2023-04-03 17:06:46,757 epoch 7 - iter 475/958 - loss 0.01091307 - time (sec): 17.96 - samples/sec: 105.77 - lr: 0.000019
2023-04-03 17:06:50,468 epoch 7 - iter 570/958 - loss 0.00912905 - time (sec): 21.67 - samples/sec: 105.19 - lr: 0.000019
2023-04-03 17:06:54,293 epoch 7 - iter 665/958 - loss 0.00789812 - time (sec): 25.50 - samples/sec: 104.32 - lr: 0.000018
2023-04-03 17:06:58,207 epoch 7 - iter 760/958 - loss 0.00702315 - time (sec): 29.41 - samples/sec: 103.36 - lr: 0.000018
2023-04-03 17:07:02,087 epoch 7 - iter 855/958 - loss 0.00629686 - time (sec): 33.29 - samples/sec: 102.73 - lr: 0.000017
2023-04-03 17:07:06,171 epoch 7 - iter 950/958 - loss 0.00735087 - time (sec): 37.38 - samples/sec: 101.67 - lr: 0.000017
2023-04-03 17:07:06,492 ----------------------------------------------------------------------------------------------------
2023-04-03 17:07:06,492 EPOCH 7 done: loss 0.0088 - lr 0.000017
2023-04-03 17:07:08,728 Evaluating as a multi-label problem: False
2023-04-03 17:07:08,735 DEV : loss 1.4802324771881104 - f1-score (micro avg)  0.8457
2023-04-03 17:07:08,741 ----------------------------------------------------------------------------------------------------
2023-04-03 17:07:12,375 epoch 8 - iter 95/958 - loss 0.02270477 - time (sec): 3.63 - samples/sec: 104.56 - lr: 0.000016
2023-04-03 17:07:16,046 epoch 8 - iter 190/958 - loss 0.02050662 - time (sec): 7.31 - samples/sec: 104.04 - lr: 0.000016
2023-04-03 17:07:19,636 epoch 8 - iter 285/958 - loss 0.03147097 - time (sec): 10.90 - samples/sec: 104.63 - lr: 0.000015
2023-04-03 17:07:23,596 epoch 8 - iter 380/958 - loss 0.02446791 - time (sec): 14.85 - samples/sec: 102.33 - lr: 0.000014
2023-04-03 17:07:27,501 epoch 8 - iter 475/958 - loss 0.02056645 - time (sec): 18.76 - samples/sec: 101.28 - lr: 0.000014
2023-04-03 17:07:31,196 epoch 8 - iter 570/958 - loss 0.01714683 - time (sec): 22.46 - samples/sec: 101.54 - lr: 0.000013
2023-04-03 17:07:34,889 epoch 8 - iter 665/958 - loss 0.01483063 - time (sec): 26.15 - samples/sec: 101.73 - lr: 0.000013
2023-04-03 17:07:38,659 epoch 8 - iter 760/958 - loss 0.01298103 - time (sec): 29.92 - samples/sec: 101.61 - lr: 0.000012
2023-04-03 17:07:42,471 epoch 8 - iter 855/958 - loss 0.01154640 - time (sec): 33.73 - samples/sec: 101.39 - lr: 0.000012
2023-04-03 17:07:46,278 epoch 8 - iter 950/958 - loss 0.01387092 - time (sec): 37.54 - samples/sec: 101.23 - lr: 0.000011
2023-04-03 17:07:46,570 ----------------------------------------------------------------------------------------------------
2023-04-03 17:07:46,570 EPOCH 8 done: loss 0.0138 - lr 0.000011
2023-04-03 17:07:48,591 Evaluating as a multi-label problem: False
2023-04-03 17:07:48,599 DEV : loss 1.3828098773956299 - f1-score (micro avg)  0.8561
2023-04-03 17:07:48,605 ----------------------------------------------------------------------------------------------------
2023-04-03 17:07:52,178 epoch 9 - iter 95/958 - loss 0.00009591 - time (sec): 3.57 - samples/sec: 106.34 - lr: 0.000011
2023-04-03 17:07:55,790 epoch 9 - iter 190/958 - loss 0.00008695 - time (sec): 7.19 - samples/sec: 105.76 - lr: 0.000010
2023-04-03 17:07:59,422 epoch 9 - iter 285/958 - loss 0.00010424 - time (sec): 10.82 - samples/sec: 105.38 - lr: 0.000009
2023-04-03 17:08:03,007 epoch 9 - iter 380/958 - loss 0.00482184 - time (sec): 14.40 - samples/sec: 105.54 - lr: 0.000009
2023-04-03 17:08:06,599 epoch 9 - iter 475/958 - loss 0.00386667 - time (sec): 17.99 - samples/sec: 105.59 - lr: 0.000008
2023-04-03 17:08:10,255 epoch 9 - iter 570/958 - loss 0.00322542 - time (sec): 21.65 - samples/sec: 105.31 - lr: 0.000008
2023-04-03 17:08:13,860 epoch 9 - iter 665/958 - loss 0.00276829 - time (sec): 25.26 - samples/sec: 105.32 - lr: 0.000007
2023-04-03 17:08:17,412 epoch 9 - iter 760/958 - loss 0.00242345 - time (sec): 28.81 - samples/sec: 105.53 - lr: 0.000007
2023-04-03 17:08:20,971 epoch 9 - iter 855/958 - loss 0.00215557 - time (sec): 32.37 - samples/sec: 105.66 - lr: 0.000006
2023-04-03 17:08:24,573 epoch 9 - iter 950/958 - loss 0.00206666 - time (sec): 35.97 - samples/sec: 105.65 - lr: 0.000006
2023-04-03 17:08:24,879 ----------------------------------------------------------------------------------------------------
2023-04-03 17:08:24,879 EPOCH 9 done: loss 0.0020 - lr 0.000006
2023-04-03 17:08:26,953 Evaluating as a multi-label problem: False
2023-04-03 17:08:26,962 DEV : loss 1.5101628303527832 - f1-score (micro avg)  0.8634
2023-04-03 17:08:26,970 ----------------------------------------------------------------------------------------------------
2023-04-03 17:08:30,506 epoch 10 - iter 95/958 - loss 0.00384982 - time (sec): 3.54 - samples/sec: 107.46 - lr: 0.000005
2023-04-03 17:08:34,038 epoch 10 - iter 190/958 - loss 0.00193593 - time (sec): 7.07 - samples/sec: 107.52 - lr: 0.000004
2023-04-03 17:08:37,602 epoch 10 - iter 285/958 - loss 0.00433985 - time (sec): 10.63 - samples/sec: 107.22 - lr: 0.000004
2023-04-03 17:08:41,159 epoch 10 - iter 380/958 - loss 0.00427237 - time (sec): 14.19 - samples/sec: 107.12 - lr: 0.000003
2023-04-03 17:08:44,698 epoch 10 - iter 475/958 - loss 0.00341875 - time (sec): 17.73 - samples/sec: 107.17 - lr: 0.000003
2023-04-03 17:08:48,242 epoch 10 - iter 570/958 - loss 0.00285989 - time (sec): 21.27 - samples/sec: 107.18 - lr: 0.000002
2023-04-03 17:08:51,890 epoch 10 - iter 665/958 - loss 0.00245222 - time (sec): 24.92 - samples/sec: 106.74 - lr: 0.000002
2023-04-03 17:08:55,609 epoch 10 - iter 760/958 - loss 0.00219219 - time (sec): 28.64 - samples/sec: 106.15 - lr: 0.000001
2023-04-03 17:08:59,324 epoch 10 - iter 855/958 - loss 0.00194964 - time (sec): 32.35 - samples/sec: 105.71 - lr: 0.000001
2023-04-03 17:09:03,005 epoch 10 - iter 950/958 - loss 0.00175519 - time (sec): 36.04 - samples/sec: 105.45 - lr: 0.000000
2023-04-03 17:09:03,306 ----------------------------------------------------------------------------------------------------
2023-04-03 17:09:03,306 EPOCH 10 done: loss 0.0017 - lr 0.000000
2023-04-03 17:09:05,465 Evaluating as a multi-label problem: False
2023-04-03 17:09:05,472 DEV : loss 1.545553207397461 - f1-score (micro avg)  0.853
2023-04-03 17:09:05,824 ----------------------------------------------------------------------------------------------------
2023-04-03 17:09:05,825 Testing using last state of model ...
2023-04-03 17:09:09,425 Evaluating as a multi-label problem: False
2023-04-03 17:09:09,433 0.8781	0.8781	0.8781	0.8781
2023-04-03 17:09:09,433 
Results:
- F-score (micro) 0.8781
- F-score (macro) 0.7953
- Accuracy 0.8781

By class:
              precision    recall  f1-score   support

           1     0.9836    1.0000    0.9918       481
           3     0.8417    0.8037    0.8223       377
           2     0.6353    0.7200    0.6750       150
           5     0.9856    1.0000    0.9928       137
           4     0.5750    0.4340    0.4946        53

    accuracy                         0.8781      1198
   macro avg     0.8042    0.7915    0.7953      1198
weighted avg     0.8775    0.8781    0.8769      1198

2023-04-03 17:09:09,433 ----------------------------------------------------------------------------------------------------
