2023-04-06 22:04:10,612 ----------------------------------------------------------------------------------------------------
2023-04-06 22:04:10,612 Model: "TextClassifier(
  (embeddings): TransformerDocumentEmbeddings(
    (model): DistilBertModel(
      (embeddings): Embeddings(
        (word_embeddings): Embedding(30523, 768)
        (position_embeddings): Embedding(512, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (transformer): Transformer(
        (layer): ModuleList(
          (0-5): 6 x TransformerBlock(
            (attention): MultiHeadSelfAttention(
              (dropout): Dropout(p=0.1, inplace=False)
              (q_lin): Linear(in_features=768, out_features=768, bias=True)
              (k_lin): Linear(in_features=768, out_features=768, bias=True)
              (v_lin): Linear(in_features=768, out_features=768, bias=True)
              (out_lin): Linear(in_features=768, out_features=768, bias=True)
            )
            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (ffn): FFN(
              (dropout): Dropout(p=0.1, inplace=False)
              (lin1): Linear(in_features=768, out_features=3072, bias=True)
              (lin2): Linear(in_features=3072, out_features=768, bias=True)
              (activation): GELUActivation()
            )
            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          )
        )
      )
    )
  )
  (decoder): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.0, inplace=False)
  (locked_dropout): LockedDropout(p=0.0)
  (word_dropout): WordDropout(p=0.0)
  (loss_function): CrossEntropyLoss()
  (weights): None
  (weight_tensor) None
)"
2023-04-06 22:04:10,612 ----------------------------------------------------------------------------------------------------
2023-04-06 22:04:10,613 Corpus: "Corpus: 8701 train + 1094 dev + 1369 test sentences"
2023-04-06 22:04:10,613 ----------------------------------------------------------------------------------------------------
2023-04-06 22:04:10,613 Parameters:
2023-04-06 22:04:10,613  - learning_rate: "0.000050"
2023-04-06 22:04:10,613  - mini_batch_size: "4"
2023-04-06 22:04:10,613  - patience: "3"
2023-04-06 22:04:10,613  - anneal_factor: "0.5"
2023-04-06 22:04:10,613  - max_epochs: "10"
2023-04-06 22:04:10,613  - shuffle: "True"
2023-04-06 22:04:10,613  - train_with_dev: "False"
2023-04-06 22:04:10,613  - batch_growth_annealing: "False"
2023-04-06 22:04:10,613 ----------------------------------------------------------------------------------------------------
2023-04-06 22:04:10,613 Model training base path: "test_distilbert_3_bucket"
2023-04-06 22:04:10,613 ----------------------------------------------------------------------------------------------------
2023-04-06 22:04:10,613 Device: cuda:0
2023-04-06 22:04:10,613 ----------------------------------------------------------------------------------------------------
2023-04-06 22:04:10,613 Embeddings storage mode: none
2023-04-06 22:04:10,613 ----------------------------------------------------------------------------------------------------
2023-04-06 22:04:22,650 epoch 1 - iter 217/2176 - loss 0.35269415 - time (sec): 12.04 - samples/sec: 72.11 - lr: 0.000005
2023-04-06 22:04:53,379 epoch 1 - iter 434/2176 - loss 0.39717260 - time (sec): 42.77 - samples/sec: 40.59 - lr: 0.000010
2023-04-06 22:05:25,906 epoch 1 - iter 651/2176 - loss 0.26478737 - time (sec): 75.29 - samples/sec: 34.58 - lr: 0.000015
2023-04-06 22:05:56,400 epoch 1 - iter 868/2176 - loss 0.19859299 - time (sec): 105.79 - samples/sec: 32.82 - lr: 0.000020
2023-04-06 22:06:46,476 epoch 1 - iter 1085/2176 - loss 0.15887519 - time (sec): 155.86 - samples/sec: 27.85 - lr: 0.000025
2023-04-06 22:07:26,080 epoch 1 - iter 1302/2176 - loss 0.15812684 - time (sec): 195.47 - samples/sec: 26.63 - lr: 0.000030
2023-04-06 22:07:58,784 epoch 1 - iter 1519/2176 - loss 0.13555243 - time (sec): 228.17 - samples/sec: 26.62 - lr: 0.000035
2023-04-06 22:08:36,752 epoch 1 - iter 1736/2176 - loss 0.11860119 - time (sec): 266.14 - samples/sec: 26.08 - lr: 0.000040
2023-04-06 22:09:17,792 epoch 1 - iter 1953/2176 - loss 0.10544530 - time (sec): 307.18 - samples/sec: 25.42 - lr: 0.000045
2023-04-06 22:09:57,750 epoch 1 - iter 2170/2176 - loss 0.09489474 - time (sec): 347.14 - samples/sec: 24.99 - lr: 0.000050
2023-04-06 22:09:58,700 ----------------------------------------------------------------------------------------------------
2023-04-06 22:09:58,700 EPOCH 1 done: loss 0.0947 - lr 0.000050
2023-04-06 22:10:01,309 Evaluating as a multi-label problem: False
2023-04-06 22:10:01,317 DEV : loss 14.187857627868652 - f1-score (micro avg)  0.3099
2023-04-06 22:10:01,322 ----------------------------------------------------------------------------------------------------
2023-04-06 22:10:38,243 epoch 2 - iter 217/2176 - loss 1.18501462 - time (sec): 36.92 - samples/sec: 23.51 - lr: 0.000049
2023-04-06 22:11:17,430 epoch 2 - iter 434/2176 - loss 0.99976361 - time (sec): 76.11 - samples/sec: 22.81 - lr: 0.000049
2023-04-06 22:11:55,502 epoch 2 - iter 651/2176 - loss 0.90315957 - time (sec): 114.18 - samples/sec: 22.81 - lr: 0.000048
2023-04-06 22:12:33,935 epoch 2 - iter 868/2176 - loss 0.83097684 - time (sec): 152.61 - samples/sec: 22.74 - lr: 0.000048
2023-04-06 22:13:13,376 epoch 2 - iter 1085/2176 - loss 0.79305007 - time (sec): 192.05 - samples/sec: 22.59 - lr: 0.000047
2023-04-06 22:13:49,909 epoch 2 - iter 1302/2176 - loss 0.75328948 - time (sec): 228.59 - samples/sec: 22.77 - lr: 0.000047
2023-04-06 22:14:28,529 epoch 2 - iter 1519/2176 - loss 0.71549608 - time (sec): 267.21 - samples/sec: 22.72 - lr: 0.000046
2023-04-06 22:15:04,736 epoch 2 - iter 1736/2176 - loss 0.68486885 - time (sec): 303.41 - samples/sec: 22.87 - lr: 0.000046
2023-04-06 22:15:42,534 epoch 2 - iter 1953/2176 - loss 0.65702627 - time (sec): 341.21 - samples/sec: 22.88 - lr: 0.000045
2023-04-06 22:16:20,189 epoch 2 - iter 2170/2176 - loss 0.63221914 - time (sec): 378.87 - samples/sec: 22.90 - lr: 0.000044
2023-04-06 22:16:21,028 ----------------------------------------------------------------------------------------------------
2023-04-06 22:16:21,028 EPOCH 2 done: loss 0.6324 - lr 0.000044
2023-04-06 22:16:23,543 Evaluating as a multi-label problem: False
2023-04-06 22:16:23,550 DEV : loss 1.6983357667922974 - f1-score (micro avg)  0.5978
2023-04-06 22:16:23,555 ----------------------------------------------------------------------------------------------------
2023-04-06 22:16:58,100 epoch 3 - iter 217/2176 - loss 0.36131589 - time (sec): 34.54 - samples/sec: 25.10 - lr: 0.000044
2023-04-06 22:17:35,905 epoch 3 - iter 434/2176 - loss 0.36012692 - time (sec): 72.35 - samples/sec: 23.95 - lr: 0.000043
2023-04-06 22:18:14,063 epoch 3 - iter 651/2176 - loss 0.35827462 - time (sec): 110.51 - samples/sec: 23.53 - lr: 0.000043
2023-04-06 22:18:50,679 epoch 3 - iter 868/2176 - loss 0.35571498 - time (sec): 147.12 - samples/sec: 23.57 - lr: 0.000042
2023-04-06 22:19:29,901 epoch 3 - iter 1085/2176 - loss 0.35650005 - time (sec): 186.35 - samples/sec: 23.27 - lr: 0.000042
2023-04-06 22:20:07,438 epoch 3 - iter 1302/2176 - loss 0.34425224 - time (sec): 223.88 - samples/sec: 23.24 - lr: 0.000041
2023-04-06 22:20:46,308 epoch 3 - iter 1519/2176 - loss 0.34385072 - time (sec): 262.75 - samples/sec: 23.11 - lr: 0.000041
2023-04-06 22:21:23,604 epoch 3 - iter 1736/2176 - loss 0.34113940 - time (sec): 300.05 - samples/sec: 23.13 - lr: 0.000040
2023-04-06 22:22:03,088 epoch 3 - iter 1953/2176 - loss 0.34077349 - time (sec): 339.53 - samples/sec: 22.99 - lr: 0.000039
2023-04-06 22:22:40,181 epoch 3 - iter 2170/2176 - loss 0.33563000 - time (sec): 376.63 - samples/sec: 23.03 - lr: 0.000039
2023-04-06 22:22:40,737 ----------------------------------------------------------------------------------------------------
2023-04-06 22:22:40,737 EPOCH 3 done: loss 0.3353 - lr 0.000039
2023-04-06 22:22:43,493 Evaluating as a multi-label problem: False
2023-04-06 22:22:43,500 DEV : loss 1.6806925535202026 - f1-score (micro avg)  0.6389
2023-04-06 22:22:43,506 ----------------------------------------------------------------------------------------------------
2023-04-06 22:23:21,190 epoch 4 - iter 217/2176 - loss 0.19458736 - time (sec): 37.68 - samples/sec: 23.03 - lr: 0.000038
2023-04-06 22:24:37,424 epoch 4 - iter 434/2176 - loss 0.21687042 - time (sec): 113.92 - samples/sec: 15.24 - lr: 0.000038
2023-04-06 22:25:15,007 epoch 4 - iter 651/2176 - loss 0.20021149 - time (sec): 151.50 - samples/sec: 17.19 - lr: 0.000037
2023-04-06 22:25:54,827 epoch 4 - iter 868/2176 - loss 0.19908604 - time (sec): 191.32 - samples/sec: 18.15 - lr: 0.000037
2023-04-06 22:26:33,224 epoch 4 - iter 1085/2176 - loss 0.20156404 - time (sec): 229.72 - samples/sec: 18.89 - lr: 0.000036
2023-04-06 22:27:12,624 epoch 4 - iter 1302/2176 - loss 0.19468747 - time (sec): 269.12 - samples/sec: 19.34 - lr: 0.000036
2023-04-06 22:27:52,892 epoch 4 - iter 1519/2176 - loss 0.19537026 - time (sec): 309.39 - samples/sec: 19.63 - lr: 0.000035
2023-04-06 22:28:31,755 epoch 4 - iter 1736/2176 - loss 0.19980475 - time (sec): 348.25 - samples/sec: 19.93 - lr: 0.000034
2023-04-06 22:29:11,215 epoch 4 - iter 1953/2176 - loss 0.20282900 - time (sec): 387.71 - samples/sec: 20.14 - lr: 0.000034
2023-04-06 22:29:51,774 epoch 4 - iter 2170/2176 - loss 0.20128948 - time (sec): 428.27 - samples/sec: 20.26 - lr: 0.000033
2023-04-06 22:29:52,859 ----------------------------------------------------------------------------------------------------
2023-04-06 22:29:52,859 EPOCH 4 done: loss 0.2008 - lr 0.000033
2023-04-06 22:29:55,658 Evaluating as a multi-label problem: False
2023-04-06 22:29:55,665 DEV : loss 1.3601137399673462 - f1-score (micro avg)  0.6682
2023-04-06 22:29:55,670 ----------------------------------------------------------------------------------------------------
2023-04-06 22:30:34,625 epoch 5 - iter 217/2176 - loss 0.11077992 - time (sec): 38.95 - samples/sec: 22.28 - lr: 0.000033
2023-04-06 22:31:13,703 epoch 5 - iter 434/2176 - loss 0.11841454 - time (sec): 78.03 - samples/sec: 22.23 - lr: 0.000032
2023-04-06 22:31:54,752 epoch 5 - iter 651/2176 - loss 0.12185108 - time (sec): 119.08 - samples/sec: 21.86 - lr: 0.000032
2023-04-06 22:32:35,595 epoch 5 - iter 868/2176 - loss 0.13359823 - time (sec): 159.92 - samples/sec: 21.70 - lr: 0.000031
2023-04-06 22:33:14,799 epoch 5 - iter 1085/2176 - loss 0.13683382 - time (sec): 199.13 - samples/sec: 21.78 - lr: 0.000031
2023-04-06 22:33:53,659 epoch 5 - iter 1302/2176 - loss 0.13815662 - time (sec): 237.99 - samples/sec: 21.87 - lr: 0.000030
2023-04-06 22:34:33,234 epoch 5 - iter 1519/2176 - loss 0.13720803 - time (sec): 277.56 - samples/sec: 21.88 - lr: 0.000029
2023-04-06 22:35:11,783 epoch 5 - iter 1736/2176 - loss 0.14051125 - time (sec): 316.11 - samples/sec: 21.95 - lr: 0.000029
2023-04-06 22:35:51,069 epoch 5 - iter 1953/2176 - loss 0.13760400 - time (sec): 355.40 - samples/sec: 21.97 - lr: 0.000028
2023-04-06 22:36:32,004 epoch 5 - iter 2170/2176 - loss 0.13585006 - time (sec): 396.33 - samples/sec: 21.89 - lr: 0.000028
2023-04-06 22:36:32,751 ----------------------------------------------------------------------------------------------------
2023-04-06 22:36:32,751 EPOCH 5 done: loss 0.1358 - lr 0.000028
2023-04-06 22:36:35,452 Evaluating as a multi-label problem: False
2023-04-06 22:36:35,461 DEV : loss 2.372546434402466 - f1-score (micro avg)  0.6179
2023-04-06 22:36:35,467 ----------------------------------------------------------------------------------------------------
2023-04-06 22:37:15,545 epoch 6 - iter 217/2176 - loss 0.08604788 - time (sec): 40.08 - samples/sec: 21.66 - lr: 0.000027
2023-04-06 22:37:54,766 epoch 6 - iter 434/2176 - loss 0.06940074 - time (sec): 79.30 - samples/sec: 21.89 - lr: 0.000027
2023-04-06 22:38:32,054 epoch 6 - iter 651/2176 - loss 0.09032859 - time (sec): 116.59 - samples/sec: 22.34 - lr: 0.000026
2023-04-06 22:39:12,055 epoch 6 - iter 868/2176 - loss 0.09218739 - time (sec): 156.59 - samples/sec: 22.17 - lr: 0.000026
2023-04-06 22:39:51,154 epoch 6 - iter 1085/2176 - loss 0.09553434 - time (sec): 195.69 - samples/sec: 22.17 - lr: 0.000025
2023-04-06 22:40:29,352 epoch 6 - iter 1302/2176 - loss 0.09572843 - time (sec): 233.89 - samples/sec: 22.25 - lr: 0.000024
2023-04-06 22:41:08,316 epoch 6 - iter 1519/2176 - loss 0.09161072 - time (sec): 272.85 - samples/sec: 22.26 - lr: 0.000024
2023-04-06 22:41:44,888 epoch 6 - iter 1736/2176 - loss 0.09136841 - time (sec): 309.42 - samples/sec: 22.43 - lr: 0.000023
2023-04-06 22:42:25,235 epoch 6 - iter 1953/2176 - loss 0.08907014 - time (sec): 349.77 - samples/sec: 22.32 - lr: 0.000023
2023-04-06 22:43:03,224 epoch 6 - iter 2170/2176 - loss 0.09162505 - time (sec): 387.76 - samples/sec: 22.37 - lr: 0.000022
2023-04-06 22:43:04,277 ----------------------------------------------------------------------------------------------------
2023-04-06 22:43:04,277 EPOCH 6 done: loss 0.0925 - lr 0.000022
2023-04-06 22:43:07,469 Evaluating as a multi-label problem: False
2023-04-06 22:43:07,477 DEV : loss 1.7468218803405762 - f1-score (micro avg)  0.6499
2023-04-06 22:43:07,483 ----------------------------------------------------------------------------------------------------
2023-04-06 22:43:45,881 epoch 7 - iter 217/2176 - loss 0.03942264 - time (sec): 38.40 - samples/sec: 22.61 - lr: 0.000022
2023-04-06 22:44:23,211 epoch 7 - iter 434/2176 - loss 0.06388631 - time (sec): 75.73 - samples/sec: 22.90 - lr: 0.000021
2023-04-06 22:45:00,252 epoch 7 - iter 651/2176 - loss 0.05632210 - time (sec): 112.77 - samples/sec: 23.07 - lr: 0.000021
2023-04-06 22:45:39,287 epoch 7 - iter 868/2176 - loss 0.06061834 - time (sec): 151.80 - samples/sec: 22.86 - lr: 0.000020
2023-04-06 22:46:15,901 epoch 7 - iter 1085/2176 - loss 0.06063487 - time (sec): 188.42 - samples/sec: 23.02 - lr: 0.000019
2023-04-06 22:46:54,926 epoch 7 - iter 1302/2176 - loss 0.06298813 - time (sec): 227.44 - samples/sec: 22.88 - lr: 0.000019
2023-04-06 22:47:34,248 epoch 7 - iter 1519/2176 - loss 0.06392008 - time (sec): 266.76 - samples/sec: 22.77 - lr: 0.000018
2023-04-06 22:48:12,316 epoch 7 - iter 1736/2176 - loss 0.06129482 - time (sec): 304.83 - samples/sec: 22.77 - lr: 0.000018
2023-04-06 22:48:50,841 epoch 7 - iter 1953/2176 - loss 0.05784649 - time (sec): 343.36 - samples/sec: 22.74 - lr: 0.000017
2023-04-06 22:49:29,890 epoch 7 - iter 2170/2176 - loss 0.06072108 - time (sec): 382.41 - samples/sec: 22.69 - lr: 0.000017
2023-04-06 22:49:30,656 ----------------------------------------------------------------------------------------------------
2023-04-06 22:49:30,656 EPOCH 7 done: loss 0.0606 - lr 0.000017
2023-04-06 22:49:33,229 Evaluating as a multi-label problem: False
2023-04-06 22:49:33,237 DEV : loss 2.212989330291748 - f1-score (micro avg)  0.6435
2023-04-06 22:49:33,243 ----------------------------------------------------------------------------------------------------
2023-04-06 22:50:12,585 epoch 8 - iter 217/2176 - loss 0.01256546 - time (sec): 39.34 - samples/sec: 22.04 - lr: 0.000016
2023-04-06 22:50:50,097 epoch 8 - iter 434/2176 - loss 0.02183088 - time (sec): 76.85 - samples/sec: 22.58 - lr: 0.000016
2023-04-06 22:51:27,840 epoch 8 - iter 651/2176 - loss 0.02524843 - time (sec): 114.60 - samples/sec: 22.70 - lr: 0.000015
2023-04-06 22:52:06,301 epoch 8 - iter 868/2176 - loss 0.03155918 - time (sec): 153.06 - samples/sec: 22.66 - lr: 0.000014
2023-04-06 22:52:45,874 epoch 8 - iter 1085/2176 - loss 0.03474361 - time (sec): 192.63 - samples/sec: 22.51 - lr: 0.000014
2023-04-06 22:53:24,594 epoch 8 - iter 1302/2176 - loss 0.03168742 - time (sec): 231.35 - samples/sec: 22.50 - lr: 0.000013
2023-04-06 22:54:03,285 epoch 8 - iter 1519/2176 - loss 0.03657568 - time (sec): 270.04 - samples/sec: 22.49 - lr: 0.000013
2023-04-06 22:54:41,393 epoch 8 - iter 1736/2176 - loss 0.03621700 - time (sec): 308.15 - samples/sec: 22.52 - lr: 0.000012
2023-04-06 22:55:18,329 epoch 8 - iter 1953/2176 - loss 0.03708421 - time (sec): 345.09 - samples/sec: 22.63 - lr: 0.000012
2023-04-06 22:55:57,202 epoch 8 - iter 2170/2176 - loss 0.03649788 - time (sec): 383.96 - samples/sec: 22.59 - lr: 0.000011
2023-04-06 22:55:57,998 ----------------------------------------------------------------------------------------------------
2023-04-06 22:55:57,998 EPOCH 8 done: loss 0.0364 - lr 0.000011
2023-04-06 22:56:00,572 Evaluating as a multi-label problem: False
2023-04-06 22:56:00,579 DEV : loss 2.2738072872161865 - f1-score (micro avg)  0.6371
2023-04-06 22:56:00,586 ----------------------------------------------------------------------------------------------------
2023-04-06 22:56:40,663 epoch 9 - iter 217/2176 - loss 0.01568114 - time (sec): 40.08 - samples/sec: 21.66 - lr: 0.000011
2023-04-06 22:57:19,785 epoch 9 - iter 434/2176 - loss 0.01854879 - time (sec): 79.20 - samples/sec: 21.92 - lr: 0.000010
2023-04-06 22:57:58,199 epoch 9 - iter 651/2176 - loss 0.01936974 - time (sec): 117.61 - samples/sec: 22.14 - lr: 0.000009
2023-04-06 22:58:35,348 epoch 9 - iter 868/2176 - loss 0.02416511 - time (sec): 154.76 - samples/sec: 22.43 - lr: 0.000009
2023-04-06 22:59:12,401 epoch 9 - iter 1085/2176 - loss 0.01945711 - time (sec): 191.82 - samples/sec: 22.62 - lr: 0.000008
2023-04-06 22:59:52,334 epoch 9 - iter 1302/2176 - loss 0.01931752 - time (sec): 231.75 - samples/sec: 22.46 - lr: 0.000008
2023-04-06 23:00:29,283 epoch 9 - iter 1519/2176 - loss 0.02013686 - time (sec): 268.70 - samples/sec: 22.60 - lr: 0.000007
2023-04-06 23:01:09,183 epoch 9 - iter 1736/2176 - loss 0.02089045 - time (sec): 308.60 - samples/sec: 22.49 - lr: 0.000007
2023-04-06 23:01:45,711 epoch 9 - iter 1953/2176 - loss 0.02082873 - time (sec): 345.12 - samples/sec: 22.62 - lr: 0.000006
2023-04-06 23:02:23,357 epoch 9 - iter 2170/2176 - loss 0.02104967 - time (sec): 382.77 - samples/sec: 22.66 - lr: 0.000006
2023-04-06 23:02:24,270 ----------------------------------------------------------------------------------------------------
2023-04-06 23:02:24,270 EPOCH 9 done: loss 0.0210 - lr 0.000006
2023-04-06 23:02:26,931 Evaluating as a multi-label problem: False
2023-04-06 23:02:26,938 DEV : loss 2.6932785511016846 - f1-score (micro avg)  0.6316
2023-04-06 23:02:26,944 ----------------------------------------------------------------------------------------------------
2023-04-06 23:03:05,541 epoch 10 - iter 217/2176 - loss 0.02502072 - time (sec): 38.60 - samples/sec: 22.44 - lr: 0.000005
2023-04-06 23:03:45,487 epoch 10 - iter 434/2176 - loss 0.02154116 - time (sec): 78.54 - samples/sec: 22.06 - lr: 0.000004
2023-04-06 23:04:24,564 epoch 10 - iter 651/2176 - loss 0.01664245 - time (sec): 117.62 - samples/sec: 22.11 - lr: 0.000004
2023-04-06 23:05:03,166 epoch 10 - iter 868/2176 - loss 0.01763837 - time (sec): 156.22 - samples/sec: 22.21 - lr: 0.000003
2023-04-06 23:05:40,822 epoch 10 - iter 1085/2176 - loss 0.01563988 - time (sec): 193.88 - samples/sec: 22.36 - lr: 0.000003
2023-04-06 23:06:20,832 epoch 10 - iter 1302/2176 - loss 0.01651901 - time (sec): 233.89 - samples/sec: 22.25 - lr: 0.000002
2023-04-06 23:06:59,557 epoch 10 - iter 1519/2176 - loss 0.01560018 - time (sec): 272.61 - samples/sec: 22.27 - lr: 0.000002
2023-04-06 23:07:35,502 epoch 10 - iter 1736/2176 - loss 0.01427003 - time (sec): 308.56 - samples/sec: 22.49 - lr: 0.000001
2023-04-06 23:08:14,115 epoch 10 - iter 1953/2176 - loss 0.01569078 - time (sec): 347.17 - samples/sec: 22.49 - lr: 0.000001
2023-04-06 23:08:53,673 epoch 10 - iter 2170/2176 - loss 0.01500874 - time (sec): 386.73 - samples/sec: 22.43 - lr: 0.000000
2023-04-06 23:08:54,551 ----------------------------------------------------------------------------------------------------
2023-04-06 23:08:54,551 EPOCH 10 done: loss 0.0150 - lr 0.000000
2023-04-06 23:08:57,318 Evaluating as a multi-label problem: False
2023-04-06 23:08:57,326 DEV : loss 2.9554803371429443 - f1-score (micro avg)  0.6271
2023-04-06 23:08:57,721 ----------------------------------------------------------------------------------------------------
2023-04-06 23:08:57,721 Testing using last state of model ...
2023-04-06 23:09:00,900 Evaluating as a multi-label problem: False
2023-04-06 23:09:00,909 0.5873	0.5873	0.5873	0.5873
2023-04-06 23:09:00,909 
Results:
- F-score (micro) 0.5873
- F-score (macro) 0.5798
- Accuracy 0.5873

By class:
              precision    recall  f1-score   support

           1     0.6654    0.6108    0.6369       573
           3     0.4879    0.7043    0.5765       372
           5     0.6275    0.4528    0.5260       424

    accuracy                         0.5873      1369
   macro avg     0.5936    0.5893    0.5798      1369
weighted avg     0.6054    0.5873    0.5862      1369

2023-04-06 23:09:00,909 ----------------------------------------------------------------------------------------------------
